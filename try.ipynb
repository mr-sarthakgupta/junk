{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter_cofrequency_matrices(words):\n",
    "        \"\"\"\n",
    "        Generate co-frequency matrices for letters that succeed and precede each other.\n",
    "        \n",
    "        Parameters:\n",
    "        words (list): List of words to analyze\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (succeeding_matrix, preceding_matrix)\n",
    "        \"\"\"\n",
    "        # Create lowercase alphabet for matrix indexing\n",
    "        alphabet = string.ascii_lowercase\n",
    "        \n",
    "        # Initialize matrices with zeros\n",
    "        succeeding_matrix = np.zeros((26, 26), dtype=int)\n",
    "        preceding_matrix = np.zeros((26, 26), dtype=int)\n",
    "        \n",
    "        # Process each word\n",
    "        for word in words:\n",
    "            # Convert to lowercase\n",
    "            word = word.lower()\n",
    "            \n",
    "            # Analyze letter successions\n",
    "            for i in range(len(word) - 1):\n",
    "                # Current and next letter\n",
    "                current_letter = word[i]\n",
    "                next_letter = word[i + 1]\n",
    "                \n",
    "                # Skip if either letter is not in alphabet\n",
    "                if current_letter not in alphabet or next_letter not in alphabet:\n",
    "                    continue\n",
    "                \n",
    "                # Get matrix indices\n",
    "                current_idx = alphabet.index(current_letter)\n",
    "                next_idx = alphabet.index(next_letter)\n",
    "                \n",
    "                # Increment succeeding matrix\n",
    "                succeeding_matrix[current_idx, next_idx] += 1\n",
    "            \n",
    "            # Analyze letter precedences\n",
    "            for i in range(1, len(word)):\n",
    "                # Current and previous letter\n",
    "                current_letter = word[i]\n",
    "                prev_letter = word[i - 1]\n",
    "                \n",
    "                # Skip if either letter is not in alphabet\n",
    "                if current_letter not in alphabet or prev_letter not in alphabet:\n",
    "                    continue\n",
    "                \n",
    "                # Get matrix indices\n",
    "                current_idx = alphabet.index(current_letter)\n",
    "                prev_idx = alphabet.index(prev_letter)\n",
    "                \n",
    "                # Increment preceding matrix\n",
    "                preceding_matrix[current_idx, prev_idx] += 1\n",
    "        \n",
    "        return succeeding_matrix, preceding_matrix\n",
    "\n",
    "\n",
    "with open(\"words_train_split.txt\", \"r\") as file:\n",
    "        words = file.read().splitlines()\n",
    "        \n",
    "succeeding_matrix, preceding_matrix = generate_letter_cofrequency_matrices(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "\n",
    "class HangmanGame:\n",
    "    def __init__(self, full_dictionary_location='words_train_split.txt'):\n",
    "\n",
    "        self.guessed_letters = []\n",
    "\n",
    "        full_dictionary_location = \"words_train_split.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)\n",
    "\n",
    "        val_dictionary_path = 'words_val_split.txt'\n",
    "        self.val_dictionary = self.build_dictionary(val_dictionary_path)\n",
    "            \n",
    "        self.alphabet = string.ascii_lowercase\n",
    "\n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []        \n",
    "\n",
    "        self.succeeding_matrix, self.preceding_matrix = succeeding_matrix, preceding_matrix\n",
    "\n",
    "    def guess(self, word, succeeding_matrix, preceding_matrix):\n",
    "        # Predefined frequency lists\n",
    "        letters_by_frequency = ['e',\n",
    "                                'i',\n",
    "                                'a',\n",
    "                                'n',\n",
    "                                'o',\n",
    "                                'r',\n",
    "                                's',\n",
    "                                't',\n",
    "                                'l',\n",
    "                                'c',\n",
    "                                'u',\n",
    "                                'd',\n",
    "                                'p',\n",
    "                                'm',\n",
    "                                'h',\n",
    "                                'g',\n",
    "                                'y',\n",
    "                                'b',\n",
    "                                'f',\n",
    "                                'v',\n",
    "                                'k',\n",
    "                                'w',\n",
    "                                'z',\n",
    "                                'x',\n",
    "                                'q',\n",
    "                                'j']\n",
    "        \n",
    "        bigrams_by_frequency = ['er',\n",
    " 'in',\n",
    " 'ti',\n",
    " 'on',\n",
    " 'es',\n",
    " 'te',\n",
    " 'an',\n",
    " 're',\n",
    " 'at',\n",
    " 'al',\n",
    " 'en',\n",
    " 'ed',\n",
    " 'le',\n",
    " 'ri',\n",
    " 'is',\n",
    " 'ra',\n",
    " 'ic',\n",
    " 'st',\n",
    " 'ar',\n",
    " 'ne',\n",
    " 'ng',\n",
    " 'li',\n",
    " 'ro',\n",
    " 'or',\n",
    " 'nt',\n",
    " 'la',\n",
    " 'un',\n",
    " 'it',\n",
    " 'co',\n",
    " 'el',\n",
    " 'de',\n",
    " 'se',\n",
    " 'll',\n",
    " 'ni',\n",
    " 'ca',\n",
    " 'to',\n",
    " 'ta',\n",
    " 'ss',\n",
    " 'io',\n",
    " 'ma',\n",
    " 'ch',\n",
    " 'ou',\n",
    " 'ia',\n",
    " 'he',\n",
    " 'lo',\n",
    " 'tr',\n",
    " 'us',\n",
    " 'no',\n",
    " 'si',\n",
    " 'ly',\n",
    " 'me',\n",
    " 'di',\n",
    " 'na',\n",
    " 'ol',\n",
    " 'et',\n",
    " 've',\n",
    " 'il',\n",
    " 'as',\n",
    " 'ac',\n",
    " 'mi',\n",
    " 'th',\n",
    " 'ea',\n",
    " 'pe',\n",
    " 'nd',\n",
    " 'ha',\n",
    " 'om',\n",
    " 'ce',\n",
    " 'os',\n",
    " 'hi',\n",
    " 'ph',\n",
    " 'ho',\n",
    " 'ur',\n",
    " 'pr',\n",
    " 'ns',\n",
    " 'id',\n",
    " 'ie',\n",
    " 'op',\n",
    " 'ul',\n",
    " 'nc',\n",
    " 'ec',\n",
    " 'ot',\n",
    " 'sh',\n",
    " 'ge',\n",
    " 'mo',\n",
    " 'pa',\n",
    " 'em',\n",
    " 'ab',\n",
    " 'po',\n",
    " 'bl',\n",
    " 'am',\n",
    " 'rs',\n",
    " 'ci',\n",
    " 'ad',\n",
    " 'pi',\n",
    " 'oc',\n",
    " 'ap',\n",
    " 'be',\n",
    " 'su',\n",
    " 'og',\n",
    " 'sa']\n",
    "        \n",
    "        trigrams_by_frequency = ['ati',\n",
    " 'tio',\n",
    " 'nes',\n",
    " 'ter',\n",
    " 'ica',\n",
    " 'all',\n",
    " 'ent',\n",
    " 'tin',\n",
    " 'non',\n",
    " 'per',\n",
    " 'eri',\n",
    " 'ver',\n",
    " 'ant',\n",
    " 'ate',\n",
    " 'abl',\n",
    " 'ali',\n",
    " 'pre',\n",
    " 'tra',\n",
    " 'lin',\n",
    " 'ing',\n",
    " 'con',\n",
    " 'nte',\n",
    " 'pro',\n",
    " 'sti',\n",
    " 'ion',\n",
    " 'nti',\n",
    " 'ste',\n",
    " 'tri',\n",
    " 'rat',\n",
    " 'ell',\n",
    " 'oni',\n",
    " 'nde',\n",
    " 'ist',\n",
    " 'res',\n",
    " 'rin',\n",
    " 'the',\n",
    " 'ari',\n",
    " 'ine',\n",
    " 'ene',\n",
    " 'ill',\n",
    " 'lat',\n",
    " 'ove',\n",
    " 'iti',\n",
    " 'lit',\n",
    " 'str',\n",
    " 'ere',\n",
    " 'ran',\n",
    " 'tic',\n",
    " 'cal',\n",
    " 'int',\n",
    " 'men',\n",
    " 'era',\n",
    " 'gra',\n",
    " 'ili',\n",
    " 'min',\n",
    " 'dis',\n",
    " 'olo',\n",
    " 'ast',\n",
    " 'ona',\n",
    " 'tro',\n",
    " 'est',\n",
    " 'ani',\n",
    " 'mat',\n",
    " 'chi',\n",
    " 'ero',\n",
    " 'sta',\n",
    " 'der',\n",
    " 'ato',\n",
    " 'and',\n",
    " 'tiv',\n",
    " 'oph',\n",
    " 'ect',\n",
    " 'her',\n",
    " 'che',\n",
    " 'und',\n",
    " 'ina',\n",
    " 'tor',\n",
    " 'for',\n",
    " 'nat',\n",
    " 'log',\n",
    " 'rea',\n",
    " 'pho',\n",
    " 'cti',\n",
    " 'ess',\n",
    " 'ori',\n",
    " 'emi',\n",
    " 'nis',\n",
    " 'cat',\n",
    " 'lli',\n",
    " 'cha',\n",
    " 'sto',\n",
    " 'ous',\n",
    " 'lis',\n",
    " 'rop',\n",
    " 'ula',\n",
    " 'par',\n",
    " 'ele',\n",
    " 'eli',\n",
    " 'les',\n",
    " 'ers']\n",
    "\n",
    "        quadgrams_by_frequency = ['atio',\n",
    " 'over',\n",
    " 'tion',\n",
    " 'nter',\n",
    " 'ical',\n",
    " 'enes',\n",
    " 'inte',\n",
    " 'call',\n",
    " 'olog',\n",
    " 'anti',\n",
    " 'tica',\n",
    " 'atin',\n",
    " 'unde',\n",
    " 'nder',\n",
    " 'rati',\n",
    " 'logi',\n",
    " 'ingl',\n",
    " 'grap',\n",
    " 'iona',\n",
    " 'ogra',\n",
    " 'ilit',\n",
    " 'isti',\n",
    " 'ther',\n",
    " 'bili',\n",
    " 'alis',\n",
    " 'ativ',\n",
    " 'enti',\n",
    " 'uper',\n",
    " 'ster',\n",
    " 'icat',\n",
    " 'lati',\n",
    " 'mati',\n",
    " 'teri',\n",
    " 'raph',\n",
    " 'supe',\n",
    " 'ment',\n",
    " 'ines',\n",
    " 'erin',\n",
    " 'ulat',\n",
    " 'stra',\n",
    " 'enta',\n",
    " 'erat',\n",
    " 'self',\n",
    " 'tati',\n",
    " 'esse',\n",
    " 'semi',\n",
    " 'snes',\n",
    " 'ight',\n",
    " 'dnes',\n",
    " 'peri',\n",
    " 'inat',\n",
    " 'pres',\n",
    " 'tran',\n",
    " 'aliz',\n",
    " 'cula',\n",
    " 'stic',\n",
    " 'tric',\n",
    " 'comp',\n",
    " 'omet',\n",
    " 'tive',\n",
    " 'ctio',\n",
    " 'vill',\n",
    " 'well',\n",
    " 'lene',\n",
    " 'tabl',\n",
    " 'ator',\n",
    " 'ecti',\n",
    " 'abil',\n",
    " 'cati',\n",
    " 'ousl',\n",
    " 'blen',\n",
    " 'nati',\n",
    " 'emen',\n",
    " 'opho',\n",
    " 'acti',\n",
    " 'able',\n",
    " 'para',\n",
    " 'lect',\n",
    " 'edne',\n",
    " 'vers',\n",
    " 'izat',\n",
    " 'cont',\n",
    " 'cons',\n",
    " 'zati',\n",
    " 'usne',\n",
    " 'asti',\n",
    " 'ousn',\n",
    " 'tero',\n",
    " 'izin',\n",
    " 'onis',\n",
    " 'anth',\n",
    " 'late',\n",
    " 'ogen',\n",
    " 'anis',\n",
    " 'rica',\n",
    " 'othe',\n",
    " 'trop',\n",
    " 'reco',\n",
    " 'elli',\n",
    " 'arch']\n",
    "        \n",
    "        # Clean the word, stripping spaces and replacing \"_\" with placeholders\n",
    "        clean_word = word[::2].replace(\"_\", \".\")\n",
    "    \n",
    "        # Score mechanism for letter selection\n",
    "        letter_scores = {}\n",
    "        \n",
    "        # 1. Single Letter Frequency - Initial Base Score\n",
    "        for letter in letters_by_frequency:\n",
    "            if letter not in self.guessed_letters:\n",
    "                letter_scores[letter] = [0]*7\n",
    "\n",
    "        # 2. Bigram and  Scoring with Contextual Constraint\n",
    "        for i in range(len(clean_word) - 1):\n",
    "            # Extract 2-letter window\n",
    "            window = clean_word[i:i+2]\n",
    "            \n",
    "            # Count known letters in the window\n",
    "            known_letters_count = sum(1 for char in window if char != '.')\n",
    "            \n",
    "            # Only apply bigram scoring if 1 or more letters are known\n",
    "            if known_letters_count == 1:\n",
    "                for bigram in bigrams_by_frequency:\n",
    "                    if self.is_bigram_window_match(window, bigram):\n",
    "                        for letter in set(bigram):\n",
    "                            if letter not in self.guessed_letters and letter not in window:\n",
    "                                letter_scores[letter][0] += bigrams_by_frequency.index(bigram) + 1\n",
    "                \n",
    "                if window[0] == '.':\n",
    "                    for letter in letters_by_frequency:\n",
    "                        if letter not in self.guessed_letters and letter not in window:\n",
    "                            letter_scores[letter][1] += list(np.argsort(-1*preceding_matrix[self.alphabet.index(window[1]), :])).index(self.alphabet.index(letter)) + 1\n",
    "                if window[1] == '.':\n",
    "                    for letter in letters_by_frequency:\n",
    "                        if letter not in self.guessed_letters and letter not in window:\n",
    "                            letter_scores[letter][2] += list(np.argsort(-1*preceding_matrix[self.alphabet.index(window[0]), :])).index(self.alphabet.index(letter)) + 1\n",
    "                                    \n",
    "        \n",
    "        # 3. Trigram Scoring with Contextual Constraint\n",
    "        for i in range(len(clean_word) - 2):\n",
    "            # Extract 3-letter window\n",
    "            window = clean_word[i:i+3]\n",
    "            \n",
    "            # Count known letters in the window\n",
    "            known_letters_count = sum(1 for char in window if char != '.')\n",
    "            \n",
    "            # Only apply trigram scoring if 2 or more letters are known\n",
    "            if known_letters_count >= 1:\n",
    "                for trigram in trigrams_by_frequency:\n",
    "                    if self.is_trigram_window_match(window, trigram):\n",
    "                        for letter in set(trigram):\n",
    "                            if letter not in self.guessed_letters and letter not in window:\n",
    "                                letter_scores[letter][3] += trigrams_by_frequency.index(trigram) + 1\n",
    "        \n",
    "            if known_letters_count >= 2:\n",
    "                for trigram in trigrams_by_frequency:\n",
    "                    if self.is_trigram_window_match(window, trigram):\n",
    "                        for letter in set(trigram):\n",
    "                            if letter not in self.guessed_letters and letter not in window:\n",
    "                                letter_scores[letter][4] += trigrams_by_frequency.index(trigram) + 1\n",
    "\n",
    "        # 4. Quadgram Scoring with Contextual Constraint\n",
    "        for i in range(len(clean_word) - 3):\n",
    "            # Extract 4-letter window\n",
    "            window = clean_word[i:i+4]\n",
    "            \n",
    "            # Count known letters in the window\n",
    "            known_letters_count = sum(1 for char in window if char != '.')\n",
    "            \n",
    "            # Only apply quadgram scoring if 2 or more letters are known\n",
    "            if known_letters_count >= 2:\n",
    "                for quadgram in quadgrams_by_frequency:\n",
    "                    if self.is_quadgram_window_match(window, quadgram):\n",
    "                        for letter in set(quadgram):\n",
    "                            if letter not in self.guessed_letters and letter not in window:\n",
    "                                letter_scores[letter][5] += quadgrams_by_frequency.index(quadgram) + 1\n",
    "\n",
    "\n",
    "            if known_letters_count >= 3:\n",
    "                for quadgram in quadgrams_by_frequency:\n",
    "                    if self.is_quadgram_window_match(window, quadgram):\n",
    "                        for letter in set(quadgram):\n",
    "                            if letter not in self.guessed_letters and letter not in window:\n",
    "                                letter_scores[letter][6] += quadgrams_by_frequency.index(quadgram) + 1\n",
    "\n",
    "        output_file_path = f'embeddings/{word.replace(\".\", \"_\")} ~ {self.secret_word}.txt'\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            for letter, scores in letter_scores.items():\n",
    "                f.write(f\"{letter}: {scores}\\n\")\n",
    "        \n",
    "        # Fallback to most frequent unguessed letters\n",
    "        if not letter_scores:\n",
    "            for letter in letters_by_frequency:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    return letter\n",
    "        \n",
    "        # Select letter with highest score\n",
    "        guess_letter = max(letter_scores, key=letter_scores.get)\n",
    "        \n",
    "        return guess_letter\n",
    "    \n",
    "    def is_bigram_window_match(self, window, bigram):\n",
    "        \"\"\"\n",
    "        Check if a bigram is compatible with a 2-letter word window\n",
    "        \n",
    "        Example matches:\n",
    "        '_ e' matches 'he'\n",
    "        'h _' matches 'hi'\n",
    "        \"\"\"\n",
    "        # Convert window to regex pattern, replacing dots with wildcards\n",
    "        pattern = '^' + ''.join([c if c != '.' else '[a-z]' for c in window]) + '$'\n",
    "        \n",
    "        return re.match(pattern, bigram, re.IGNORECASE) is not None\n",
    "\n",
    "    def is_trigram_window_match(self, window, trigram):\n",
    "        \"\"\"\n",
    "        Check if a trigram is compatible with a 3-letter word window\n",
    "        \n",
    "        Example matches:\n",
    "        '_ _ e' matches 'the'\n",
    "        'h e _' matches 'her'\n",
    "        \"\"\"\n",
    "        # Convert window to regex pattern, replacing dots with wildcards\n",
    "        pattern = '^' + ''.join([c if c != '.' else '[a-z]' for c in window]) + '$'\n",
    "        \n",
    "        return re.match(pattern, trigram, re.IGNORECASE) is not None\n",
    "\n",
    "    def is_quadgram_window_match(self, window, quadgram):\n",
    "        \"\"\"\n",
    "        Check if a quadgram is compatible with a 4-letter word window\n",
    "        \n",
    "        Example matches:\n",
    "        '_ _ e r' matches 'ther'\n",
    "        'w i t _' matches 'with'\n",
    "        \"\"\"\n",
    "        # Convert window to regex pattern, replacing dots with wildcards\n",
    "        pattern = '^' + ''.join([c if c != '.' else '[a-z]' for c in window]) + '$'\n",
    "\n",
    "        return re.match(pattern, quadgram, re.IGNORECASE) is not None\n",
    "\n",
    "    def is_ngram_compatible(self, ngram, word_pattern):\n",
    "        \"\"\"\n",
    "        Check if an ngram is compatible with the current word pattern\n",
    "        \"\"\"\n",
    "        # Create a regex pattern from the ngram that respects the word pattern\n",
    "        pattern = word_pattern.replace('.', '[a-z]')\n",
    "        \n",
    "        # Check if the ngram could exist within the pattern\n",
    "        return re.search(f'(?=.{ngram}.)', pattern, re.IGNORECASE) is not None\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "        \n",
    "    def start_game(self, secret_word=None, verbose=True):\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "        if secret_word is None:\n",
    "            secret_word = random.choice(self.val_dictionary)\n",
    "        self.secret_word = secret_word\n",
    "        word = ' '.join(['_' for _ in secret_word])\n",
    "        tries_remains = 6\n",
    "        # if verbose:\n",
    "            # print(\"Successfully start a new game! # of tries remaining: {0}. Word: {1}.\".format(tries_remains, word))\n",
    "        while tries_remains > 0:\n",
    "            # get guessed letter from user code\n",
    "            guess_letter = self.guess(word, self.succeeding_matrix, self.preceding_matrix)\n",
    "            \n",
    "            # append guessed letter to guessed letters field in hangman object\n",
    "            self.guessed_letters.append(guess_letter)\n",
    "            # if verbose:\n",
    "                # print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "\n",
    "            if guess_letter in secret_word:\n",
    "                # update word with guessed letter\n",
    "                word = ' '.join([letter if letter == guess_letter else word[index*2] for index, letter in enumerate(secret_word)])\n",
    "            if guess_letter not in secret_word:\n",
    "                tries_remains -= 1\n",
    "\n",
    "            if tries_remains > 0:\n",
    "                status = 'ongoing'\n",
    "            if '_' not in word:\n",
    "                status = 'success'            \n",
    "            if tries_remains == 0:\n",
    "                status = 'failed'\n",
    "            res = {'status': status, 'tries_remains': tries_remains, 'word': word}\n",
    "            # print(res)\n",
    "\n",
    "            if status == 'success':\n",
    "                # print(\"Successfully finished game, the word was: {0}!\".format(secret_word))\n",
    "                return True\n",
    "\n",
    "            if status == 'failed':\n",
    "                # print(\"Failed game because of: # of tries exceeded!\")\n",
    "                return False\n",
    "            \n",
    "        return status==\"success\"\n",
    "    \n",
    "game = HangmanGame()\n",
    "game.start_game(secret_word='welder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m----> 6\u001b[0m     game \u001b[38;5;241m=\u001b[39m HangmanGame()\n\u001b[0;32m      7\u001b[0m     game\u001b[38;5;241m.\u001b[39mstart_game(secret_word\u001b[38;5;241m=\u001b[39mword)\n",
      "Cell \u001b[1;32mIn[84], line 22\u001b[0m, in \u001b[0;36mHangmanGame.__init__\u001b[1;34m(self, full_dictionary_location)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_dictionary(val_dictionary_path)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mascii_lowercase\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary_common_letter_sorted \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary))\u001b[38;5;241m.\u001b[39mmost_common()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary \u001b[38;5;241m=\u001b[39m []        \n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msucceeding_matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreceding_matrix \u001b[38;5;241m=\u001b[39m succeeding_matrix, preceding_matrix\n",
      "File \u001b[1;32mc:\\Users\\mrsar\\miniconda3\\envs\\mark\\Lib\\collections\\__init__.py:599\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m \n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(iterable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"words_train_split.txt\", \"r\") as file:\n",
    "        words = file.read().splitlines()\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    print(i)\n",
    "    game = HangmanGame()\n",
    "    game.start_game(secret_word=word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
